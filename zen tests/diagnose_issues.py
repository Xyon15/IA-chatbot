#!/usr/bin/env python3
"""
Script de diagnostic pour identifier et corriger les probl√®mes du bot Neuro
"""

import os
import sys
from pathlib import Path
import json

def check_discord_token():
    """V√©rifie et valide le token Discord"""
    print("üîê V√©rification du token Discord...")
    
    env_file = Path('.env')
    if not env_file.exists():
        print("‚ùå Fichier .env manquant")
        return False
    
    with open(env_file, 'r') as f:
        content = f.read()
    
    # V√©rifier si le token est pr√©sent
    if 'DISCORD_TOKEN=' not in content:
        print("‚ùå DISCORD_TOKEN manquant dans .env")
        return False
    
    # Extraire le token
    for line in content.split('\n'):
        if line.startswith('DISCORD_TOKEN='):
            token = line.split('=', 1)[1].strip()
            if not token or token == 'votre_token_discord_ici':
                print("‚ùå Token Discord vide ou exemple")
                print("üîß Solution: Remplacez par un vrai token Discord")
                print("   1. Allez sur https://discord.com/developers/applications")
                print("   2. Cr√©ez une nouvelle application")
                print("   3. Dans Bot > Token, copiez le token")
                print("   4. Remplacez la valeur dans .env")
                return False
            
            # V√©rifier le format basique du token
            if len(token) < 50:
                print(f"‚ùå Token trop court: {len(token)} caract√®res")
                print("üîß Un token Discord valide fait g√©n√©ralement 70+ caract√®res")
                return False
            
            print(f"‚úÖ Token pr√©sent ({len(token)} caract√®res)")
            return True
    
    return False

def check_gpu_configuration():
    """V√©rifie la configuration GPU"""
    print("üñ•Ô∏è V√©rification de la configuration GPU...")
    
    try:
        import pynvml
        pynvml.nvmlInit()
        gpu_count = pynvml.nvmlDeviceGetCount()
        print(f"‚úÖ {gpu_count} GPU(s) NVIDIA d√©tect√©(s)")
        
        # Afficher les informations des GPUs
        for i in range(gpu_count):
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)
            name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
            memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
            print(f"   GPU {i}: {name} - M√©moire: {memory.total // 1024**2} MB")
        
        return True
        
    except ImportError:
        print("‚ö†Ô∏è pynvml non install√© - impossible de d√©tecter les GPUs")
        print("   Installation: pip install pynvml")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur GPU: {e}")
        print("   Le bot utilisera le CPU")
        return False

def check_llama_cpp_gpu():
    """V√©rifie que llama-cpp-python supporte CUDA"""
    print("üîß V√©rification de llama-cpp-python avec support CUDA...")
    
    try:
        from llama_cpp import Llama
        print("‚úÖ llama-cpp-python install√©")
        
        # Tenter de cr√©er un mod√®le minimal avec GPU
        print("üß™ Test de configuration GPU...")
        
        # V√©rifier si nous avons un mod√®le de test
        models_dir = Path("models")
        model_files = list(models_dir.glob("*.gguf")) if models_dir.exists() else []
        
        if model_files:
            print(f"üìÑ Mod√®les trouv√©s: {[m.name for m in model_files]}")
            return True
        else:
            print("‚ùå Aucun mod√®le .gguf trouv√© dans le dossier models/")
            print("üîß Solution: T√©l√©chargez un mod√®le GGUF compatible")
            return False
            
    except ImportError:
        print("‚ùå llama-cpp-python non install√©")
        print("üîß Installation avec support CUDA:")
        print("   pip uninstall llama-cpp-python")
        print("   pip install llama-cpp-python --force-reinstall --no-cache-dir")
        return False
    except Exception as e:
        print(f"‚ùå Erreur llama-cpp: {e}")
        return False

def create_env_template():
    """Cr√©e un fichier .env template avec de vraies instructions"""
    print("üìù Cr√©ation du fichier .env template...")
    
    env_content = """# Configuration du bot Neuro-Bot
# IMPORTANT: Remplacez les valeurs ci-dessous par vos vraies valeurs

# Token de votre bot Discord (obligatoire)
# 1. Allez sur https://discord.com/developers/applications
# 2. Cr√©ez une nouvelle application
# 3. Dans Bot > Token, copiez le token
# 4. Remplacez la ligne ci-dessous:
DISCORD_TOKEN=VOTRE_VRAI_TOKEN_ICI

# Secret pour l'authentification 2FA (optionnel mais recommand√©)
# G√©n√©rez une cl√© secr√®te de 32 caract√®res
# Vous pouvez utiliser: https://www.allkeysgenerator.com/Random/Security-Encryption-Key-Generator.aspx
AUTH_SECRET=VOTRE_SECRET_2FA_ICI

# Configuration avanc√©e (optionnel)
# N_GPU_LAYERS=32  # Nombre de couches sur GPU (0 = CPU uniquement)
# N_THREADS=6      # Nombre de threads CPU
# N_CTX=4096       # Taille du contexte
# N_BATCH=256      # Taille des batches
"""
    
    # Sauvegarder le template
    with open('.env.template', 'w') as f:
        f.write(env_content)
    
    print("‚úÖ Template cr√©√©: .env.template")
    print("üîß Copiez .env.template vers .env et modifiez les valeurs")

def fix_gpu_config():
    """Propose des corrections pour la configuration GPU"""
    print("üîß Correction de la configuration GPU...")
    
    env_file = Path('.env')
    if env_file.exists():
        with open(env_file, 'r') as f:
            content = f.read()
        
        # V√©rifier si N_GPU_LAYERS est d√©fini
        if 'N_GPU_LAYERS=' not in content:
            print("‚öôÔ∏è Ajout de N_GPU_LAYERS √† .env...")
            with open(env_file, 'a') as f:
                f.write('\n# Configuration GPU\n')
                f.write('N_GPU_LAYERS=32  # Utilisez 0 pour CPU uniquement\n')
            print("‚úÖ N_GPU_LAYERS ajout√©")
        
        print("üìã Configuration GPU recommand√©e:")
        print("   N_GPU_LAYERS=32   # Pour GPU NVIDIA")
        print("   N_GPU_LAYERS=0    # Pour CPU uniquement")

def test_model_loading():
    """Teste le chargement d'un mod√®le"""
    print("üß™ Test de chargement de mod√®le...")
    
    try:
        from config import config
        model_path = Path(config.MODEL_PATH)
        
        if not model_path.exists():
            print(f"‚ùå Mod√®le non trouv√©: {model_path}")
            return False
        
        print(f"üìÑ Mod√®le trouv√©: {model_path.name}")
        print(f"üìè Taille: {model_path.stat().st_size // 1024**2} MB")
        
        # Test de chargement minimal
        from llama_cpp import Llama
        
        print("üîÑ Test de chargement (peut prendre quelques minutes)...")
        test_config = {
            'n_gpu_layers': 1,  # Test avec 1 couche seulement
            'n_ctx': 512,       # Contexte r√©duit pour test
            'verbose': False
        }
        
        try:
            llm = Llama(model_path=str(model_path), **test_config)
            print("‚úÖ Mod√®le charg√© avec succ√®s")
            
            # Test de g√©n√©ration simple
            result = llm("Test", max_tokens=1)
            print("‚úÖ G√©n√©ration de texte fonctionnelle")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur de chargement: {e}")
            if "CUDA" in str(e):
                print("üîß Probl√®me CUDA d√©tect√© - essayez N_GPU_LAYERS=0")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur lors du test: {e}")
        return False

def main():
    """Fonction principale de diagnostic"""
    print("üîç Neuro-Bot - Diagnostic des Probl√®mes")
    print("=" * 50)
    
    # V√©rifications par ordre de priorit√©
    issues_found = []
    
    # 1. Token Discord
    if not check_discord_token():
        issues_found.append("Token Discord invalide")
        create_env_template()
    
    # 2. Configuration GPU
    if not check_gpu_configuration():
        issues_found.append("Probl√®me de d√©tection GPU")
    
    # 3. llama-cpp-python avec CUDA
    if not check_llama_cpp_gpu():
        issues_found.append("Probl√®me llama-cpp-python")
    
    # 4. Configuration
    fix_gpu_config()
    
    # 5. Test de mod√®le (optionnel)
    print("\n" + "="*50)
    test_model = input("ü§î Voulez-vous tester le chargement du mod√®le ? (y/N): ").lower()
    if test_model == 'y':
        test_model_loading()
    
    # R√©sum√©
    print("\n" + "="*50)
    print("üìä R√âSUM√â DU DIAGNOSTIC")
    
    if issues_found:
        print("‚ùå Probl√®mes d√©tect√©s:")
        for issue in issues_found:
            print(f"   ‚Ä¢ {issue}")
        
        print("\nüîß ACTIONS RECOMMAND√âES:")
        print("1. Corrigez le token Discord dans .env")
        print("2. Si pas de GPU: ajoutez N_GPU_LAYERS=0 dans .env")  
        print("3. R√©installez llama-cpp-python si n√©cessaire")
        print("4. Relancez le diagnostic apr√®s corrections")
    else:
        print("‚úÖ Configuration semble correcte")
        print("üöÄ Vous pouvez essayer de lancer le bot")

if __name__ == "__main__":
    main()