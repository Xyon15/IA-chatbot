#!/usr/bin/env python3
"""
Script pour installer llama-cpp-python avec support CUDA
"""

import subprocess
import sys
import os
from pathlib import Path

def check_cuda_installed():
    """V√©rifie si CUDA est install√© sur le syst√®me"""
    print("üîç V√©rification de CUDA...")
    
    # V√©rifier nvcc
    try:
        result = subprocess.run(['nvcc', '--version'], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("‚úÖ CUDA Toolkit d√©tect√©")
            print(f"   Version: {result.stdout}")
            return True
    except (subprocess.TimeoutExpired, FileNotFoundError):
        pass
    
    # V√©rifier les variables d'environnement CUDA
    cuda_path = os.environ.get('CUDA_PATH')
    if cuda_path and Path(cuda_path).exists():
        print(f"‚úÖ CUDA_PATH trouv√©: {cuda_path}")
        return True
    
    print("‚ùå CUDA Toolkit non d√©tect√©")
    return False

def install_cuda_llama():
    """Installe llama-cpp-python avec support CUDA"""
    print("üì¶ Installation de llama-cpp-python avec CUDA...")
    
    # Commandes d'installation
    commands = [
        # D√©sinstaller la version actuelle
        [sys.executable, '-m', 'pip', 'uninstall', 'llama-cpp-python', '-y'],
        
        # R√©installer avec CUDA (force les variables d'environnement)
        [sys.executable, '-m', 'pip', 'install', 'llama-cpp-python', 
         '--force-reinstall', '--no-cache-dir']
    ]
    
    # Variables d'environnement pour forcer CUDA
    env = os.environ.copy()
    env['CMAKE_ARGS'] = '-DLLAMA_CUBLAS=on'
    env['FORCE_CMAKE'] = '1'
    
    for i, cmd in enumerate(commands, 1):
        print(f"üîÑ √âtape {i}/{len(commands)}: {' '.join(cmd)}")
        
        try:
            if i == 2:  # Deuxi√®me commande avec variables d'environnement
                result = subprocess.run(cmd, env=env, check=True, timeout=600)
            else:
                result = subprocess.run(cmd, check=True, timeout=600)
            
            print(f"‚úÖ √âtape {i} r√©ussie")
            
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Erreur lors de l'√©tape {i}: {e}")
            return False
        except subprocess.TimeoutExpired:
            print(f"‚ùå Timeout lors de l'√©tape {i}")
            return False
    
    return True

def test_cuda_support():
    """Teste si CUDA fonctionne avec llama-cpp-python"""
    print("üß™ Test du support CUDA...")
    
    try:
        from llama_cpp import Llama
        
        # Trouver un mod√®le de test
        models_dir = Path("models")
        model_files = list(models_dir.glob("*.gguf")) if models_dir.exists() else []
        
        if not model_files:
            print("‚ùå Aucun mod√®le .gguf trouv√© pour le test")
            return False
        
        model_path = model_files[0]
        print(f"üìÑ Test avec: {model_path.name}")
        
        # Test avec GPU
        print("‚è≥ Chargement avec GPU (peut prendre quelques minutes)...")
        test_llm = Llama(
            model_path=str(model_path),
            n_gpu_layers=1,  # Une couche sur GPU
            n_ctx=256,       # Contexte r√©duit pour test rapide
            verbose=True
        )
        
        print("‚úÖ CUDA fonctionne!")
        
        # Test de g√©n√©ration
        result = test_llm("Test", max_tokens=1)
        print("‚úÖ G√©n√©ration avec GPU r√©ussie")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Test CUDA √©chou√©: {e}")
        return False

def update_env_for_gpu():
    """Met √† jour .env pour utiliser le GPU"""
    print("‚öôÔ∏è Configuration pour utiliser le GPU...")
    
    env_file = Path('.env')
    if not env_file.exists():
        print("‚ùå Fichier .env non trouv√©")
        return False
    
    with open(env_file, 'r') as f:
        lines = f.readlines()
    
    # Modifier N_GPU_LAYERS
    updated = False
    for i, line in enumerate(lines):
        if line.strip().startswith('N_GPU_LAYERS='):
            lines[i] = 'N_GPU_LAYERS=32  # GPU NVIDIA activ√©\n'
            updated = True
            break
    
    if not updated:
        lines.append('\nN_GPU_LAYERS=32  # GPU NVIDIA activ√©\n')
    
    # Sauvegarder
    with open(env_file, 'w') as f:
        f.writelines(lines)
    
    print("‚úÖ Configuration GPU activ√©e dans .env")
    return True

def show_cuda_installation_guide():
    """Affiche le guide d'installation CUDA"""
    print("üìñ Guide d'Installation CUDA")
    print("=" * 40)
    print()
    print("Pour installer CUDA Toolkit:")
    print("1. Allez sur: https://developer.nvidia.com/cuda-downloads")
    print("2. S√©lectionnez: Windows > x86_64 > 11 > exe (local)")
    print("3. T√©l√©chargez et installez CUDA Toolkit")
    print("4. Red√©marrez votre ordinateur")
    print("5. Relancez ce script pour installer llama-cpp-python")
    print()
    print("‚ö†Ô∏è ATTENTION:")
    print("- L'installation CUDA prend ~3GB d'espace disque")
    print("- N√©cessite un red√©marrage")
    print("- Compatible uniquement avec GPU NVIDIA")

def main():
    """Fonction principale"""
    print("üöÄ Installation Support CUDA - Neuro-Bot")
    print("=" * 50)
    
    # V√©rifier CUDA
    if not check_cuda_installed():
        print()
        show_cuda_installation_guide()
        
        install_cuda = input("\n‚ùì CUDA n'est pas install√©. Voulez-vous voir le guide d'installation ? (y/N): ").lower()
        if install_cuda == 'y':
            show_cuda_installation_guide()
        return
    
    # CUDA est install√©, proc√©der √† l'installation
    print("\nüîß CUDA d√©tect√©. Installation de llama-cpp-python avec support GPU...")
    proceed = input("Continuer ? Cela va r√©installer llama-cpp-python (y/N): ").lower()
    
    if proceed != 'y':
        print("‚ùå Installation annul√©e")
        return
    
    # Installation
    if install_cuda_llama():
        print("\n‚úÖ Installation termin√©e!")
        
        # Test
        test_cuda = input("\nüß™ Tester le support CUDA ? (y/N): ").lower()
        if test_cuda == 'y':
            if test_cuda_support():
                print("\nüéâ Support CUDA fonctionnel!")
                
                # Mettre √† jour .env
                update_gpu = input("\n‚öôÔ∏è Activer le GPU dans la configuration ? (y/N): ").lower()
                if update_gpu == 'y':
                    update_env_for_gpu()
                    print("\nüöÄ Configuration compl√®te! Vous pouvez lancer: python start_bot.py")
                else:
                    print("\nüí° Pour activer le GPU plus tard, changez N_GPU_LAYERS=32 dans .env")
            else:
                print("\n‚ùå Probl√®me avec CUDA. Le bot utilisera le CPU.")
        else:
            print("\nüí° Pour tester plus tard, relancez ce script")
    else:
        print("\n‚ùå √âchec de l'installation. Le bot continuera √† utiliser le CPU.")

if __name__ == "__main__":
    main()