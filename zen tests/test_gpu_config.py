#!/usr/bin/env python3
"""
Test et correction de la configuration GPU pour le bot Neuro
"""

import os
import sys
from pathlib import Path

def test_pynvml():
    """Teste pynvml avec gestion des erreurs"""
    print("üîß Test d√©taill√© de pynvml...")
    
    try:
        import pynvml
        print("‚úÖ pynvml import√©")
        
        # Initialisation
        pynvml.nvmlInit()
        print("‚úÖ pynvml initialis√©")
        
        # Obtenir le nombre de GPUs
        gpu_count = pynvml.nvmlDeviceGetCount()
        print(f"‚úÖ {gpu_count} GPU(s) d√©tect√©(s)")
        
        # Pour chaque GPU
        for i in range(gpu_count):
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                print(f"‚úÖ Handle GPU {i} obtenu")
                
                # Nom du GPU (correction du bug decode)
                try:
                    name = pynvml.nvmlDeviceGetName(handle)
                    if isinstance(name, bytes):
                        name = name.decode('utf-8')
                    elif isinstance(name, str):
                        pass  # D√©j√† une cha√Æne
                    else:
                        name = str(name)
                    print(f"‚úÖ GPU {i}: {name}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Impossible d'obtenir le nom du GPU {i}: {e}")
                    name = f"GPU {i} (nom inconnu)"
                
                # Informations m√©moire
                try:
                    memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
                    total_mb = memory.total // 1024**2
                    free_mb = memory.free // 1024**2
                    used_mb = memory.used // 1024**2
                    print(f"   M√©moire: {used_mb}/{total_mb} MB utilis√©s ({free_mb} MB libres)")
                except Exception as e:
                    print(f"‚ö†Ô∏è Impossible d'obtenir les infos m√©moire du GPU {i}: {e}")
                
                # Temp√©rature
                try:
                    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                    print(f"   Temp√©rature: {temp}¬∞C")
                except Exception as e:
                    print(f"‚ö†Ô∏è Impossible d'obtenir la temp√©rature du GPU {i}: {e}")
                
            except Exception as e:
                print(f"‚ùå Erreur pour GPU {i}: {e}")
        
        return True
        
    except ImportError:
        print("‚ùå pynvml non install√©")
        print("üîß Installation: pip install pynvml")
        return False
    except Exception as e:
        print(f"‚ùå Erreur pynvml: {e}")
        return False

def test_cuda_availability():
    """Teste la disponibilit√© de CUDA"""
    print("\nüîß Test de disponibilit√© CUDA...")
    
    # Test via llama-cpp-python
    try:
        from llama_cpp import Llama
        
        # Cr√©er une instance de test avec 1 couche GPU
        print("üß™ Test avec 1 couche GPU...")
        models_dir = Path("models")
        model_files = list(models_dir.glob("*.gguf"))
        
        if not model_files:
            print("‚ùå Aucun mod√®le trouv√© pour le test")
            return False
        
        model_path = model_files[0]  # Premier mod√®le trouv√©
        print(f"üìÑ Utilisation du mod√®le: {model_path.name}")
        
        # Configuration de test avec GPU
        test_config = {
            'model_path': str(model_path),
            'n_gpu_layers': 1,  # Une seule couche pour test rapide
            'n_ctx': 512,       # Contexte r√©duit
            'n_batch': 64,      # Batch r√©duit
            'verbose': True     # Pour voir les messages CUDA
        }
        
        print("‚è≥ Chargement du mod√®le avec GPU (peut prendre 1-2 minutes)...")
        llm = Llama(**test_config)
        print("‚úÖ Mod√®le charg√© avec GPU activ√©")
        
        # Test de g√©n√©ration tr√®s simple
        print("üß™ Test de g√©n√©ration...")
        result = llm("Hello", max_tokens=5, temperature=0)
        print("‚úÖ G√©n√©ration r√©ussie avec GPU")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur CUDA: {e}")
        
        if "CUDA" in str(e) or "cuBLAS" in str(e):
            print("üîß Probl√®me CUDA d√©tect√©. Solutions:")
            print("   1. V√©rifiez que CUDA est install√©")
            print("   2. R√©installez llama-cpp-python avec support CUDA")
            print("   3. Ou utilisez N_GPU_LAYERS=0 pour CPU uniquement")
        
        return False

def fix_config_for_cpu():
    """Configure le bot pour utiliser uniquement le CPU"""
    print("\nüîß Configuration pour CPU uniquement...")
    
    env_file = Path('.env')
    if not env_file.exists():
        print("‚ùå Fichier .env manquant")
        return False
    
    with open(env_file, 'r') as f:
        lines = f.readlines()
    
    # Modifier N_GPU_LAYERS
    modified = False
    for i, line in enumerate(lines):
        if line.strip().startswith('N_GPU_LAYERS='):
            lines[i] = 'N_GPU_LAYERS=0  # CPU uniquement\n'
            modified = True
            break
    
    if not modified:
        lines.append('\nN_GPU_LAYERS=0  # CPU uniquement\n')
    
    # Sauvegarder
    with open(env_file, 'w') as f:
        f.writelines(lines)
    
    print("‚úÖ Configuration mise √† jour pour CPU uniquement")
    return True

def test_final_config():
    """Test final de la configuration"""
    print("\nüß™ Test de la configuration finale...")
    
    try:
        from config import config
        print(f"‚úÖ Configuration charg√©e")
        print(f"   TOKEN: {'‚úÖ Pr√©sent' if config.TOKEN else '‚ùå Manquant'}")
        print(f"   N_GPU_LAYERS: {config.LLM_CONFIG.get('n_gpu_layers', 'Non d√©fini')}")
        print(f"   Mod√®le: {Path(config.MODEL_PATH).name}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erreur de configuration: {e}")
        return False

def main():
    """Fonction principale"""
    print("üîß Test et Correction GPU - Neuro-Bot")
    print("=" * 50)
    
    # Test 1: pynvml
    gpu_ok = test_pynvml()
    
    # Test 2: CUDA uniquement si GPU d√©tect√©
    cuda_ok = False
    if gpu_ok:
        cuda_ok = test_cuda_availability()
    
    # Si CUDA ne fonctionne pas, configurer pour CPU
    if not cuda_ok:
        print("\n‚ö†Ô∏è GPU/CUDA non fonctionnel - configuration pour CPU")
        fix_config_for_cpu()
    
    # Test final
    test_final_config()
    
    # Recommandations finales
    print("\n" + "=" * 50)
    print("üìã RECOMMANDATIONS FINALES:")
    
    if cuda_ok:
        print("‚úÖ Configuration GPU fonctionnelle")
        print("üöÄ Le bot peut utiliser votre GPU NVIDIA")
    else:
        print("‚ö†Ô∏è Configuration CPU activ√©e")
        print("üêå Le bot sera plus lent mais fonctionnel")
        print("üîß Pour activer GPU plus tard:")
        print("   1. Installez CUDA Toolkit")
        print("   2. pip uninstall llama-cpp-python")
        print("   3. pip install llama-cpp-python --force-reinstall --no-cache-dir")
        print("   4. Changez N_GPU_LAYERS=32 dans .env")
    
    print("\nüöÄ Vous pouvez maintenant lancer: python start_bot.py")

if __name__ == "__main__":
    main()