#!/usr/bin/env python3
"""
Script pour activer le support GPU pour Neuro-Bot
"""

import subprocess
import sys
import os
from pathlib import Path

def test_current_cuda_support():
    """Teste si la version actuelle de llama-cpp-python supporte CUDA"""
    print("üß™ Test du support CUDA actuel...")
    
    try:
        from llama_cpp import Llama
        
        # Test avec un mod√®le minimal et GPU
        models_dir = Path("models")
        model_files = list(models_dir.glob("*.gguf"))
        
        if not model_files:
            print("‚ùå Aucun mod√®le trouv√© pour le test")
            return False
        
        model_path = model_files[0]
        print(f"üìÑ Test avec: {model_path.name}")
        
        # Essayer de charger avec GPU
        print("‚è≥ Test de chargement avec 1 couche GPU...")
        test_llm = Llama(
            model_path=str(model_path),
            n_gpu_layers=1,
            n_ctx=256,
            n_batch=64,
            verbose=False
        )
        
        # V√©rifier si des couches sont sur GPU
        print("‚úÖ Chargement r√©ussi - v√©rification de l'utilisation GPU...")
        
        # Test de g√©n√©ration simple
        result = test_llm("Test", max_tokens=1, temperature=0)
        print("‚úÖ Support CUDA d√©j√† fonctionnel !")
        return True
        
    except Exception as e:
        if "CUDA" in str(e) or "cuBLAS" in str(e) or "GPU" in str(e):
            print(f"‚ùå Support CUDA non disponible: {e}")
        else:
            print(f"‚ùå Erreur lors du test: {e}")
        return False

def install_cuda_llama_cpp():
    """Installe llama-cpp-python avec support CUDA"""
    print("\nüîß Installation de llama-cpp-python avec support CUDA...")
    print("‚ö†Ô∏è Cela va prendre plusieurs minutes...")
    
    # Variables d'environnement pour forcer CUDA
    env = os.environ.copy()
    env['CMAKE_ARGS'] = '-DLLAMA_CUBLAS=on'
    env['FORCE_CMAKE'] = '1'
    
    commands = [
        # D√©sinstaller la version actuelle
        [sys.executable, '-m', 'pip', 'uninstall', 'llama-cpp-python', '-y'],
        
        # Nettoyer le cache pip
        [sys.executable, '-m', 'pip', 'cache', 'purge'],
        
        # R√©installer avec CUDA
        [sys.executable, '-m', 'pip', 'install', 'llama-cpp-python', 
         '--force-reinstall', '--no-cache-dir', '--verbose']
    ]
    
    for i, cmd in enumerate(commands, 1):
        print(f"\nüîÑ √âtape {i}/{len(commands)}")
        print(f"Commande: {' '.join(cmd)}")
        
        try:
            if i == 3:  # Installation avec CUDA
                print("üì¶ Installation avec support CUDA (cela peut prendre 5-10 minutes)...")
                result = subprocess.run(cmd, env=env, check=True, timeout=1800)  # 30 minutes max
            else:
                result = subprocess.run(cmd, check=True, timeout=300)  # 5 minutes max
            
            print(f"‚úÖ √âtape {i} termin√©e avec succ√®s")
            
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Erreur lors de l'√©tape {i}:")
            print(f"Code de retour: {e.returncode}")
            if hasattr(e, 'output') and e.output:
                print(f"Sortie: {e.output}")
            return False
            
        except subprocess.TimeoutExpired:
            print(f"‚ùå Timeout lors de l'√©tape {i} (processus trop long)")
            return False
        
        except KeyboardInterrupt:
            print(f"‚ùå Installation interrompue par l'utilisateur")
            return False
    
    return True

def update_gpu_config():
    """Met √† jour la configuration pour utiliser le GPU"""
    print("\n‚öôÔ∏è Mise √† jour de la configuration GPU...")
    
    env_file = Path('.env')
    if not env_file.exists():
        print("‚ùå Fichier .env non trouv√©")
        return False
    
    # Lire le fichier actuel
    with open(env_file, 'r') as f:
        lines = f.readlines()
    
    # Modifier N_GPU_LAYERS
    updated = False
    for i, line in enumerate(lines):
        if line.strip().startswith('N_GPU_LAYERS='):
            # Pour RTX 4050 avec 6GB VRAM, on peut utiliser 28-32 couches
            lines[i] = 'N_GPU_LAYERS=32  # GPU RTX 4050 activ√©\n'
            updated = True
            break
    
    if not updated:
        lines.append('\nN_GPU_LAYERS=32  # GPU RTX 4050 activ√©\n')
    
    # Ajouter des commentaires sur l'optimisation GPU
    gpu_config = """
# Configuration GPU optimis√©e pour RTX 4050
# N_GPU_LAYERS=32   # Toutes les couches sur GPU (maximum performance)
# N_GPU_LAYERS=28   # La plupart des couches sur GPU (s√©curis√©)
# N_GPU_LAYERS=16   # Hybride GPU/CPU (√©conomise la VRAM)
# N_GPU_LAYERS=0    # CPU uniquement (pas de VRAM utilis√©e)
"""
    
    # Ajouter la configuration si elle n'existe pas d√©j√†
    content = ''.join(lines)
    if 'Configuration GPU optimis√©e' not in content:
        lines.append(gpu_config)
    
    # Sauvegarder
    with open(env_file, 'w') as f:
        f.writelines(lines)
    
    print("‚úÖ Configuration GPU mise √† jour dans .env")
    print("üéØ N_GPU_LAYERS=32 (toutes les couches sur GPU)")
    return True

def final_gpu_test():
    """Test final avec le GPU activ√©"""
    print("\nüöÄ Test final avec GPU activ√©...")
    
    try:
        from llama_cpp import Llama
        
        # Trouver un mod√®le
        models_dir = Path("models")
        model_files = list(models_dir.glob("*.gguf"))
        
        if not model_files:
            print("‚ùå Aucun mod√®le trouv√©")
            return False
        
        model_path = model_files[0]
        print(f"üìÑ Test avec: {model_path.name}")
        
        # Chargement avec GPU complet
        print("‚è≥ Chargement avec GPU (32 couches)...")
        llm = Llama(
            model_path=str(model_path),
            n_gpu_layers=32,
            n_ctx=1024,
            n_batch=128,
            verbose=True  # Pour voir les messages CUDA
        )
        
        print("\nüß™ Test de g√©n√©ration...")
        result = llm("Bonjour", max_tokens=10, temperature=0.7)
        
        print("‚úÖ Test GPU r√©ussi !")
        print(f"R√©ponse: {result['choices'][0]['text'].strip()}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Test GPU √©chou√©: {e}")
        return False

def main():
    """Fonction principale"""
    print("üöÄ Activation du Support GPU - Neuro-Bot")
    print("=" * 50)
    print(f"üéØ GPU Cible: RTX 4050 Laptop")
    print(f"üîß CUDA Version: 12.9")
    
    # Test du support actuel
    if test_current_cuda_support():
        print("\nüéâ Le support CUDA est d√©j√† fonctionnel !")
        update_gpu_config()
        print("\n‚úÖ Configuration termin√©e. Le bot utilisera maintenant le GPU !")
        return
    
    print("\nüîß Support CUDA non disponible - installation n√©cessaire")
    
    # Confirmer l'installation
    confirm = input("\n‚ùì Installer llama-cpp-python avec support CUDA ? (y/N): ").lower()
    if confirm != 'y':
        print("‚ùå Installation annul√©e")
        return
    
    print("\n‚ö†Ô∏è AVERTISSEMENT:")
    print("- L'installation peut prendre 10-15 minutes")
    print("- N√©cessite une connexion internet stable")
    print("- Le processus va compiler les binaires CUDA")
    
    proceed = input("\nContinuer ? (y/N): ").lower()
    if proceed != 'y':
        print("‚ùå Installation annul√©e")
        return
    
    # Installation
    if install_cuda_llama_cpp():
        print("\nüéâ Installation termin√©e avec succ√®s !")
        
        # Mise √† jour config
        update_gpu_config()
        
        # Test final
        print("\nüß™ Test de la configuration GPU...")
        if final_gpu_test():
            print("\nüéâ SUCC√àS ! Le bot peut maintenant utiliser votre GPU RTX 4050 !")
            print("üöÄ Performance attendue: 20-50 tokens/seconde (vs 1-5 en CPU)")
            print("\nüí° Commandes utiles:")
            print("   python start_bot.py  # Lancer le bot avec GPU")
            print("   !stats               # Voir l'utilisation GPU sur Discord")
        else:
            print("\n‚ö†Ô∏è Installation r√©ussie mais test √©chou√©")
            print("Le bot devrait quand m√™me fonctionner avec GPU")
    else:
        print("\n‚ùå √âchec de l'installation")
        print("Le bot continuera √† utiliser le CPU")

if __name__ == "__main__":
    main()