#!/usr/bin/env python3
"""
Tests pour le module model.py - Focus sur la fonction generate_reply et le bug n_ctx
"""

import sys
import asyncio
import unittest.mock as mock
from pathlib import Path

# Test imports
def test_imports():
    """Test des imports du module model"""
    print("üß™ Test des imports du module model...")
    
    try:
        from model import ModelManager, model_manager, generate_reply
        from config import config, logger
        from utils import count_tokens, truncate_text_to_tokens, shorten_response
        from memory import save_interaction, get_history, get_facts
        print("‚úÖ Imports du module model r√©ussis")
        return True
    except Exception as e:
        print(f"‚ùå Erreur d'import: {e}")
        return False

def test_generate_successful_response():
    """Test de g√©n√©ration d'une r√©ponse r√©ussie"""
    print("\nüß™ Test de g√©n√©ration d'une r√©ponse r√©ussie...")
    
    try:
        from model import generate_reply, model_manager
        
        # Mock du mod√®le et des d√©pendances
        with mock.patch.object(model_manager, 'is_ready', return_value=True):
            with mock.patch('model.llm') as mock_llm:
                # Configuration du mock llm avec n_ctx en tant qu'entier (cas normal)
                mock_llm.n_ctx = 4096
                mock_llm.return_value = {
                    "choices": [{"text": "Bonjour ! Comment √ßa va ?"}]
                }
                
                with mock.patch('model.get_history', return_value=[]):
                    with mock.patch('model.get_facts', return_value=[]):
                        with mock.patch('model.count_tokens', return_value=100):
                            with mock.patch('model.save_interaction'):
                                with mock.patch('model.shorten_response', return_value="Bonjour ! Comment √ßa va ?"):
                                    
                                    result = asyncio.run(generate_reply("user123", "Salut !", 5))
                                    
                                    assert isinstance(result, str), "La r√©ponse doit √™tre une cha√Æne"
                                    assert len(result) > 0, "La r√©ponse ne doit pas √™tre vide"
                                    assert "‚ùå" not in result, "La r√©ponse ne doit pas contenir d'erreur"
                                    
        print("‚úÖ Test de g√©n√©ration r√©ussie pass√©")
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors du test de g√©n√©ration r√©ussie: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_handle_n_ctx_as_method():
    """Test de gestion de n_ctx en tant que m√©thode (BUG IDENTIFI√â)"""
    print("\nüß™ Test de gestion de n_ctx en tant que m√©thode (BUG)...")
    
    try:
        from model import generate_reply, model_manager
        
        # Mock du mod√®le avec n_ctx comme m√©thode au lieu d'un entier
        with mock.patch.object(model_manager, 'is_ready', return_value=True):
            with mock.patch('model.llm') as mock_llm:
                # Simulation du bug: n_ctx est une m√©thode, pas un entier
                mock_llm.n_ctx = lambda: 4096  # M√©thode au lieu d'entier
                mock_llm.return_value = {
                    "choices": [{"text": "R√©ponse de test"}]
                }
                
                with mock.patch('model.get_history', return_value=[]):
                    with mock.patch('model.get_facts', return_value=[]):
                        with mock.patch('model.count_tokens', return_value=100):
                            with mock.patch('model.save_interaction'):
                                with mock.patch('model.shorten_response', return_value="R√©ponse de test"):
                                    
                                    result = asyncio.run(generate_reply("user123", "Test", 5))
                                    
                                    # Ce test devrait √©chouer avec le bug actuel
                                    # Si on arrive ici sans erreur, le bug est corrig√©
                                    if "‚ùå Erreur lors de la g√©n√©ration:" in result:
                                        print("üêõ Bug confirm√©: n_ctx en tant que m√©thode cause une erreur")
                                        return False
                                    else:
                                        print("‚úÖ Bug corrig√©: n_ctx m√©thode g√©r√©e correctement")
                                        return True
                                    
    except TypeError as e:
        if "'<=' not supported between instances of 'int' and 'method'" in str(e):
            print("üêõ Bug confirm√©: erreur de comparaison int/m√©thode d√©tect√©e")
            return False
        else:
            print(f"‚ùå Erreur inattendue: {e}")
            return False
    except Exception as e:
        print(f"‚ùå Erreur lors du test n_ctx m√©thode: {e}")
        return False

def test_handle_model_not_ready():
    """Test de gestion quand le mod√®le n'est pas pr√™t"""
    print("\nüß™ Test de gestion du mod√®le non pr√™t...")
    
    try:
        from model import generate_reply, model_manager
        
        # Mock du mod√®le non pr√™t
        with mock.patch.object(model_manager, 'is_ready', return_value=False):
            result = asyncio.run(generate_reply("user123", "Test", 5))
            
            assert "‚ùå Mod√®le non initialis√©" == result, "Message d'erreur incorrect"
            
        print("‚úÖ Test mod√®le non pr√™t pass√©")
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors du test mod√®le non pr√™t: {e}")
        return False

def test_handle_empty_prompt():
    """Test de gestion d'un prompt vide"""
    print("\nüß™ Test de gestion d'un prompt vide...")
    
    try:
        from model import generate_reply, model_manager
        
        with mock.patch.object(model_manager, 'is_ready', return_value=True):
            with mock.patch('model.llm') as mock_llm:
                mock_llm.n_ctx = 4096
                mock_llm.return_value = {
                    "choices": [{"text": "Je n'ai pas de question √† traiter."}]
                }
                
                with mock.patch('model.get_history', return_value=[]):
                    with mock.patch('model.get_facts', return_value=[]):
                        with mock.patch('model.count_tokens', return_value=50):
                            with mock.patch('model.save_interaction'):
                                with mock.patch('model.shorten_response', return_value="Je n'ai pas de question √† traiter."):
                                    
                                    result = asyncio.run(generate_reply("user123", "", 5))
                                    
                                    assert isinstance(result, str), "La r√©ponse doit √™tre une cha√Æne"
                                    assert len(result) > 0, "La r√©ponse ne doit pas √™tre vide"
                                    
        print("‚úÖ Test prompt vide pass√©")
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors du test prompt vide: {e}")
        return False

def test_handle_token_limit_exceeded():
    """Test de gestion du d√©passement de limite de tokens"""
    print("\nüß™ Test de gestion du d√©passement de limite de tokens...")
    
    try:
        from model import generate_reply, model_manager
        
        with mock.patch.object(model_manager, 'is_ready', return_value=True):
            with mock.patch('model.llm') as mock_llm:
                mock_llm.n_ctx = 4096
                
                with mock.patch('model.get_history', return_value=[]):
                    with mock.patch('model.get_facts', return_value=[]):
                        # Simuler un prompt tr√®s long (plus de tokens que disponible)
                        with mock.patch('model.count_tokens', return_value=4500):  # > 4096
                            with mock.patch('model.truncate_text_to_tokens') as mock_truncate:
                                mock_truncate.return_value = "Prompt tronqu√©"
                                with mock.patch('model.count_tokens', return_value=100):  # Apr√®s troncature
                                    mock_llm.return_value = {
                                        "choices": [{"text": "R√©ponse apr√®s troncature"}]
                                    }
                                    
                                    with mock.patch('model.save_interaction'):
                                        with mock.patch('model.shorten_response', return_value="R√©ponse apr√®s troncature"):
                                            
                                            result = asyncio.run(generate_reply("user123", "Prompt tr√®s long", 5))
                                            
                                            # V√©rifier que la troncature a √©t√© appel√©e
                                            mock_truncate.assert_called_once()
                                            assert "‚ùå" not in result, "Ne devrait pas y avoir d'erreur apr√®s troncature"
                                            
        print("‚úÖ Test d√©passement tokens pass√©")
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors du test d√©passement tokens: {e}")
        return False

def test_reduce_context_when_needed():
    """Test de r√©duction du contexte quand n√©cessaire"""
    print("\nüß™ Test de r√©duction du contexte...")
    
    try:
        from model import generate_reply, model_manager
        
        with mock.patch.object(model_manager, 'is_ready', return_value=True):
            with mock.patch('model.llm') as mock_llm:
                mock_llm.n_ctx = 1000  # Contexte limit√©
                mock_llm.return_value = {
                    "choices": [{"text": "R√©ponse avec contexte r√©duit"}]
                }
                
                # Simuler un historique qui force la r√©duction du contexte
                long_history = [("Message tr√®s long " * 50, "R√©ponse tr√®s longue " * 50)] * 10
                with mock.patch('model.get_history', return_value=long_history):
                    with mock.patch('model.get_facts', return_value=[]):
                        # Premier appel: tokens √©lev√©s, for√ßant la r√©duction
                        with mock.patch('model.count_tokens', side_effect=[900, 800, 700, 200]):
                            with mock.patch('model.save_interaction'):
                                with mock.patch('model.shorten_response', return_value="R√©ponse avec contexte r√©duit"):
                                    
                                    result = asyncio.run(generate_reply("user123", "Test", 10))
                                    
                                    assert "‚ùå" not in result, "Ne devrait pas y avoir d'erreur"
                                    
        print("‚úÖ Test r√©duction contexte pass√©")
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors du test r√©duction contexte: {e}")
        return False

def test_handle_llm_generation_error():
    """Test de gestion d'erreur lors de la g√©n√©ration LLM"""
    print("\nüß™ Test de gestion d'erreur lors de la g√©n√©ration...")
    
    try:
        from model import generate_reply, model_manager
        
        with mock.patch.object(model_manager, 'is_ready', return_value=True):
            with mock.patch('model.llm') as mock_llm:
                mock_llm.n_ctx = 4096
                # Simuler une erreur lors de la g√©n√©ration
                mock_llm.side_effect = RuntimeError("Erreur de g√©n√©ration du mod√®le")
                
                with mock.patch('model.get_history', return_value=[]):
                    with mock.patch('model.get_facts', return_value=[]):
                        with mock.patch('model.count_tokens', return_value=100):
                            
                            result = asyncio.run(generate_reply("user123", "Test", 5))
                            
                            assert "‚ùå Erreur lors de la g√©n√©ration:" in result, "Message d'erreur attendu"
                            
        print("‚úÖ Test erreur g√©n√©ration pass√©")
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors du test erreur g√©n√©ration: {e}")
        return False

def main():
    """Fonction principale de test"""
    print("üöÄ Tests du module model.py - Focus sur generate_reply")
    print("=" * 60)
    
    tests = [
        test_imports,
        test_generate_successful_response,
        test_handle_n_ctx_as_method,  # Test du bug principal
        test_handle_model_not_ready,
        test_handle_empty_prompt,
        test_handle_token_limit_exceeded,
        test_reduce_context_when_needed,
        test_handle_llm_generation_error
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if test():
                passed += 1
        except Exception as e:
            print(f"‚ùå Test {test.__name__} √©chou√©: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 60)
    print(f"üìä R√©sultats: {passed}/{total} tests r√©ussis")
    
    if passed == total:
        print("üéâ Tous les tests sont pass√©s !")
        return 0
    else:
        print("‚ö†Ô∏è Certains tests ont √©chou√©.")
        print("üí° Le test 'test_handle_n_ctx_as_method' devrait √©chouer tant que le bug n'est pas corrig√©.")
        return 1

if __name__ == "__main__":
    sys.exit(main())