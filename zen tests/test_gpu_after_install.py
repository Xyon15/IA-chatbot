#!/usr/bin/env python3
"""
Test du support GPU aprÃ¨s rÃ©installation de llama-cpp-python
"""

import sys
from pathlib import Path

def test_gpu_support():
    """Teste le support GPU aprÃ¨s rÃ©installation"""
    print("ğŸ§ª Test du Support GPU - AprÃ¨s RÃ©installation")
    print("=" * 50)
    
    try:
        from llama_cpp import Llama
        print("âœ… llama-cpp-python importÃ© avec succÃ¨s")
        
        # Trouver un modÃ¨le de test
        models_dir = Path("models")
        model_files = list(models_dir.glob("*.gguf"))
        
        if not model_files:
            print("âŒ Aucun modÃ¨le .gguf trouvÃ©")
            return False
        
        model_path = model_files[0]  # Premier modÃ¨le trouvÃ©
        print(f"ğŸ“„ Test avec: {model_path.name}")
        
        # Test avec 4 couches GPU pour commencer
        print("\nğŸ”§ Test avec 4 couches sur GPU...")
        print("â³ Chargement du modÃ¨le (peut prendre 1-2 minutes)...")
        
        llm = Llama(
            model_path=str(model_path),
            n_gpu_layers=4,  # Commencer avec quelques couches
            n_ctx=512,       # Contexte rÃ©duit pour test rapide
            n_batch=64,      # Batch rÃ©duit
            verbose=True     # Voir les messages de chargement
        )
        
        print("\nğŸ‰ ModÃ¨le chargÃ© avec succÃ¨s !")
        
        # Test de gÃ©nÃ©ration simple
        print("\nğŸ§ª Test de gÃ©nÃ©ration...")
        response = llm("Hello", max_tokens=5, temperature=0)
        print(f"âœ… GÃ©nÃ©ration rÃ©ussie: {response['choices'][0]['text'].strip()}")
        
        # VÃ©rifier si des couches sont sur GPU dans les logs
        print("\nğŸ“Š Recherche des messages GPU dans les logs ci-dessus...")
        print("Cherchez des lignes contenant 'GPU' ou 'CUDA' dans la sortie verbose")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lors du test: {e}")
        
        if "CUDA" in str(e) or "cuBLAS" in str(e):
            print("\nğŸ”§ ProblÃ¨me CUDA dÃ©tectÃ©:")
            print("- VÃ©rifiez que CUDA Toolkit est bien installÃ©")
            print("- La rÃ©installation de llama-cpp-python pourrait Ãªtre nÃ©cessaire")
            print("- Ou utilisez N_GPU_LAYERS=0 pour CPU uniquement")
        
        return False

def check_gpu_assignment():
    """VÃ©rifie l'assignation des couches GPU dans les logs"""
    print("\nğŸ” Guide de VÃ©rification GPU:")
    print("Dans les logs ci-dessus, cherchez:")
    print("âœ… 'layer X assigned to device GPU' = Couches sur GPU")
    print("âŒ 'layer X assigned to device CPU' = Couches sur CPU")
    print("âœ… Messages contenant 'CUDA' ou 'cuBLAS' = Support GPU actif")
    print("âœ… 'GPU KV buffer size' = MÃ©moire GPU utilisÃ©e")

if __name__ == "__main__":
    success = test_gpu_support()
    check_gpu_assignment()
    
    if success:
        print("\nğŸ‰ Test rÃ©ussi ! Le GPU semble fonctionnel.")
        print("ğŸ’¡ Vous pouvez maintenant augmenter N_GPU_LAYERS dans .env")
    else:
        print("\nâŒ Test Ã©chouÃ©. Le bot utilisera le CPU.")
        print("ğŸ’¡ Changez N_GPU_LAYERS=0 dans .env pour CPU uniquement")