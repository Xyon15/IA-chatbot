from auth_decorators import require_authorized_role
from model import model_manager, LLM_PROFILES
import pynvml
import os

def setup(bot):
    @bot.command()
    @require_authorized_role
    async def optimize(ctx, action: str = None, profile: str = None):
        """
        Commande d'optimisation du contexte LLM
        Usage: !optimize [analyze|apply|profiles]
        """
        
        if action is None:
            help_msg = (
                "```\n"
                "ğŸ”§ OPTIMISATION GPU AVANCÃ‰E\n"
                "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
                "!optimize analyze      - Analyser l'utilisation GPU/VRAM\n"
                "!optimize apply        - Appliquer l'optimisation recommandÃ©e\n"
                "!optimize profiles     - Afficher les profils disponibles\n"
                "!optimize current      - Afficher le profil actuel\n"
                "!optimize set <profil> - Changer de profil\n"
                "!optimize report       - Rapport d'optimisation dÃ©taillÃ©\n"
                "!optimize metrics      - MÃ©triques de performance temps rÃ©el\n"
                "!optimize auto on/off  - Optimisation automatique\n"
                "!optimize task <type>  - Optimiser pour un type de tÃ¢che\n"
                "                        (chat, batch, long_context)\n"
                "```"
            )
            await ctx.send(help_msg)
            return
        
        try:
            action_lower = action.lower()
            if action_lower == "analyze":
                await analyze_vram(ctx)
            elif action_lower == "apply":
                await apply_optimization(ctx)
            elif action_lower == "profiles":
                await show_profiles(ctx)
            elif action_lower == "current":
                await show_current_profile(ctx)
            elif action_lower == "set":
                # Supporte !optimize set <profil> via second argument
                if profile:
                    await set_profile(ctx, profile)
                else:
                    await ctx.send("âŒ SpÃ©cifiez le profil: `!optimize set <profil>`")
            elif action_lower.startswith("set"):
                # Compat: supporte aussi !optimize "set turbo_max" en un seul argument
                parts = action.split()
                if len(parts) > 1:
                    profile_name = parts[1]
                    await set_profile(ctx, profile_name)
                else:
                    await ctx.send("âŒ SpÃ©cifiez le profil: `!optimize set <profil>`")
            elif action_lower == "report":
                await show_optimization_report(ctx)
            elif action_lower == "metrics":
                await show_performance_metrics(ctx)
            elif action_lower == "auto":
                auto_state = profile.lower() if profile else None
                await toggle_auto_optimization(ctx, auto_state)
            elif action_lower == "task":
                task_type = profile if profile else "chat"
                await optimize_for_task(ctx, task_type)
            else:
                available_actions = "analyze, apply, profiles, current, set, report, metrics, auto, task"
                await ctx.send(f"âŒ Action inconnue: `{action}`\nActions disponibles: {available_actions}")
                
        except Exception as e:
            await ctx.send(f"âŒ Erreur lors de l'optimisation: {e}")

async def analyze_vram(ctx):
    """Analyse l'utilisation VRAM actuelle"""
    try:
        # RÃ©cupÃ©rer les informations du modÃ¨le
        current_profile = model_manager.get_current_profile()
        recommended_profile_key = model_manager.get_recommended_profile()
        
        if not current_profile or not current_profile['gpu_info']:
            await ctx.send("âŒ Informations GPU non disponibles")
            return
        
        gpu_info = current_profile['gpu_info']
        gpu_name = gpu_info['name']
        vram_used = gpu_info['vram_used_mb']
        vram_total = gpu_info['vram_total_mb']
        vram_free = gpu_info['vram_free_mb']
        vram_percent = gpu_info['vram_usage_percent']
        
        # Analyse du statut
        if vram_percent > 90:
            status = "ğŸ”´ CRITIQUE"
            recommendation = "RÃ©duction immÃ©diate nÃ©cessaire"
        elif vram_percent > 80:
            status = "ğŸŸ  Ã‰LEVÃ‰E"
            recommendation = "Surveiller et considÃ©rer une rÃ©duction"
        elif vram_percent > 60:
            status = "ğŸŸ¡ MODÃ‰RÃ‰E"
            recommendation = "Utilisation normale"
        else:
            status = "ğŸŸ¢ OPTIMALE"
            recommendation = "PossibilitÃ© d'augmenter les performances"
        
        # Profil recommandÃ©
        recommended_profile = LLM_PROFILES[recommended_profile_key]
        current_profile_name = current_profile['config']['name']
        
        msg = (
            "```\n"
            "ğŸ“Š ANALYSE VRAM POUR OPTIMISATION\n"
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            f"ğŸ–¥ï¸ GPU: {gpu_name}\n"
            f"ğŸ§® VRAM: {vram_used} MB / {vram_total} MB ({vram_percent:.1f}%)\n"
            f"ğŸ’¾ VRAM libre: {vram_free} MB\n"
            f"ğŸ“ˆ Statut: {status}\n"
            f"ğŸ’¡ Recommandation: {recommendation}\n"
            "\n"
            "ğŸ¯ PROFILS\n"
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            f"ğŸ“‹ Profil actuel: {current_profile_name}\n"
            f"ğŸ¯ Profil recommandÃ©: {recommended_profile['name']}\n"
            f"ğŸ§® Contexte recommandÃ©: {recommended_profile['n_ctx']:,} tokens\n"
            "\n"
            "Utilisez !optimize apply pour appliquer\n"
            "ou !optimize profiles pour voir tous les profils\n"
            "```"
        )
        
        await ctx.send(msg)
        
    except Exception as e:
        await ctx.send(f"âŒ Erreur lors de l'analyse VRAM: {e}")

async def apply_optimization(ctx):
    """Applique l'optimisation recommandÃ©e"""
    try:
        # Obtenir le profil recommandÃ©
        current_profile_key = model_manager.current_profile
        recommended_profile_key = model_manager.get_recommended_profile()
        
        if current_profile_key == recommended_profile_key:
            await ctx.send("âœ… Configuration dÃ©jÃ  optimale - aucun changement nÃ©cessaire")
            return
        
        current_profile_name = LLM_PROFILES[current_profile_key]['name']
        recommended_profile_name = LLM_PROFILES[recommended_profile_key]['name']
        
        # Appliquer le changement de profil
        success = model_manager.change_profile(recommended_profile_key)
        
        if success:
            msg = (
                f"âœ… **Optimisation appliquÃ©e avec succÃ¨s**\n"
                f"```\n"
                f"ğŸ“‹ Ancien profil: {current_profile_name}\n"
                f"ğŸ¯ Nouveau profil: {recommended_profile_name}\n"
                f"ğŸ§® Nouveau contexte: {LLM_PROFILES[recommended_profile_key]['n_ctx']:,} tokens\n"
                f"ğŸ”§ Couches GPU: {LLM_PROFILES[recommended_profile_key]['n_gpu_layers']}\n"
                f"ğŸ“¦ Batch size: {LLM_PROFILES[recommended_profile_key]['n_batch']}\n"
                f"```\n"
                f"âœ… **Changement appliquÃ© immÃ©diatement - pas de redÃ©marrage nÃ©cessaire**"
            )
        else:
            msg = (
                f"âŒ **Ã‰chec de l'optimisation**\n"
                f"Le profil {recommended_profile_name} n'a pas pu Ãªtre appliquÃ©.\n"
                f"Le profil {current_profile_name} a Ã©tÃ© restaurÃ©."
            )
        
        await ctx.send(msg)
        
    except Exception as e:
        await ctx.send(f"âŒ Erreur lors de l'application: {e}")

async def show_profiles(ctx):
    """Affiche tous les profils de configuration disponibles"""
    profiles = model_manager.get_available_profiles()
    current_profile_key = model_manager.current_profile
    
    msg_parts = [
        "```\nğŸ¯ PROFILS DE CONFIGURATION DISPONIBLES\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
    ]
    
    profile_emojis = {
        'performance_max': 'ğŸš€',
        'balanced': 'âš–ï¸',
        'economical': 'ğŸ’¾',
        'emergency': 'ğŸ†˜',
        'cpu_only': 'ğŸ–¥ï¸'
    }
    
    for key, profile in profiles.items():
        emoji = profile_emojis.get(key, 'ğŸ“‹')
        current_marker = " (ACTUEL)" if key == current_profile_key else ""
        
        msg_parts.extend([
            f"\n{emoji} {profile['name'].upper()}{current_marker}\n",
            f"   Contexte: {profile['n_ctx']:,} tokens\n",
            f"   Couches GPU: {profile['n_gpu_layers']}\n",
            f"   Batch: {profile['n_batch']}\n",
            f"   VRAM min libre: {profile['min_vram_free_mb']} MB\n",
            f"   Usage: {profile['description']}\n"
        ])
    
    msg_parts.extend([
        "\nğŸ’¡ Commandes utiles:\n",
        "   !optimize analyze - Voir le profil recommandÃ©\n",
        "   !optimize set <profil> - Changer de profil\n",
        "   !optimize current - Voir le profil actuel\n",
        "```"
    ])
    
    await ctx.send(''.join(msg_parts))

async def show_current_profile(ctx):
    """Affiche le profil actuellement utilisÃ©"""
    try:
        current_profile = model_manager.get_current_profile()
        context_info = model_manager.get_context_info()
        
        if not current_profile:
            await ctx.send("âŒ Impossible de rÃ©cupÃ©rer le profil actuel")
            return
        
        profile = current_profile['config']
        gpu_info = current_profile['gpu_info']
        
        msg = (
            "```\n"
            "ğŸ¯ PROFIL ACTUEL\n"
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            f"ğŸ“‹ Nom: {profile['name']}\n"
            f"ğŸ“ Description: {profile['description']}\n"
            f"ğŸ§® Contexte: {profile['n_ctx']:,} tokens\n"
            f"ğŸ”§ Couches GPU: {profile['n_gpu_layers']}\n"
            f"ğŸ“¦ Batch size: {profile['n_batch']}\n"
            f"ğŸ§µ Threads: {profile['n_threads']}\n"
        )
        
        if context_info:
            msg += f"âš¡ EfficacitÃ©: {context_info['efficiency_percent']:.1f}%\n"
        
        if gpu_info:
            msg += (
                f"\nğŸ–¥ï¸ GPU: {gpu_info['name']}\n"
                f"ğŸ§® VRAM: {gpu_info['vram_used_mb']} MB / {gpu_info['vram_total_mb']} MB\n"
                f"ğŸ’¾ VRAM libre: {gpu_info['vram_free_mb']} MB\n"
            )
        
        msg += "```"
        
        await ctx.send(msg)
        
    except Exception as e:
        await ctx.send(f"âŒ Erreur lors de l'affichage du profil: {e}")

async def set_profile(ctx, profile_name):
    """Change le profil de configuration"""
    try:
        # Mapper les noms courts aux clÃ©s (mis Ã  jour avec nouveaux profils)
        profile_mapping = {
            'turbo': 'turbo_max',
            'max': 'turbo_max',
            'performance': 'performance_optimized',
            'perf': 'performance_optimized',
            'stable': 'stable_high',
            'high': 'stable_high',
            'balanced': 'balanced_adaptive',
            'equilibre': 'balanced_adaptive',
            'adaptive': 'balanced_adaptive',
            'conservative': 'conservative_stable',
            'conservateur': 'conservative_stable',
            'economical': 'conservative_stable',
            'eco': 'conservative_stable',
            'emergency': 'emergency_safe',
            'secours': 'emergency_safe',
            'safe': 'emergency_safe',
            'cpu': 'cpu_fallback',
            'fallback': 'cpu_fallback',
            # CompatibilitÃ© anciens noms
            'performance_max': 'performance_max',
            'turbo_4050': 'turbo_max'
        }
        
        profile_key = profile_mapping.get(profile_name.lower(), profile_name.lower())
        
        if profile_key not in LLM_PROFILES:
            available = ', '.join(profile_mapping.keys())
            await ctx.send(f"âŒ Profil inconnu: `{profile_name}`\nProfils disponibles: {available}")
            return
        
        current_profile_key = model_manager.current_profile
        if current_profile_key == profile_key:
            await ctx.send(f"âœ… Le profil `{LLM_PROFILES[profile_key]['name']}` est dÃ©jÃ  actif")
            return
        
        # Changer le profil
        success = model_manager.change_profile(profile_key)
        
        if success:
            new_profile = LLM_PROFILES[profile_key]
            msg = (
                f"âœ… **Profil changÃ© avec succÃ¨s**\n"
                f"```\n"
                f"ğŸ¯ Nouveau profil: {new_profile['name']}\n"
                f"ğŸ§® Contexte: {new_profile['n_ctx']:,} tokens\n"
                f"ğŸ”§ Couches GPU: {new_profile['n_gpu_layers']}\n"
                f"ğŸ“¦ Batch size: {new_profile['n_batch']}\n"
                f"```\n"
                f"âœ… **Changement appliquÃ© immÃ©diatement**"
            )
        else:
            msg = f"âŒ **Ã‰chec du changement de profil**\nImpossible d'appliquer le profil `{LLM_PROFILES[profile_key]['name']}`"
        
        await ctx.send(msg)
        
    except Exception as e:
        await ctx.send(f"âŒ Erreur lors du changement de profil: {e}")

async def show_optimization_report(ctx):
    """Affiche un rapport d'optimisation dÃ©taillÃ©"""
    try:
        report = model_manager.get_gpu_optimization_report()
        
        if "error" in report:
            await ctx.send(f"âŒ {report['error']}")
            if "fallback_info" in report:
                info = report["fallback_info"]
                msg = (
                    f"```\n"
                    f"ğŸ“‹ INFORMATIONS BASIQUES\n"
                    f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
                    f"Profil actuel: {info['profile_name']}\n"
                    f"ClÃ© profil: {info['current_profile']}\n"
                )
                if info.get("gpu_info"):
                    gpu = info["gpu_info"]
                    msg += (
                        f"GPU: {gpu['name']}\n"
                        f"VRAM: {gpu['vram_used_mb']} MB / {gpu['vram_total_mb']} MB\n"
                    )
                msg += "```"
                await ctx.send(msg)
            return
        
        # Formatage du rapport complet
        current = report["current_metrics"]
        averages = report["averages_last_5min"]
        optimal = report["optimal_profile"]
        recommendations = report["recommendations"]
        monitoring = report["monitoring_status"]
        
        msg = (
            f"```\n"
            f"ğŸš€ RAPPORT D'OPTIMISATION GPU DÃ‰TAILLÃ‰\n"
            f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
            f"\n"
            f"ğŸ–¥ï¸ GPU ACTUELLE\n"
            f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            f"GPU: {current['gpu_name']}\n"
            f"VRAM: {current['vram_usage']}\n"
            f"TempÃ©rature: {current['temperature']}\n"
            f"Consommation: {current['power_usage']}\n"
            f"Utilisation GPU: {current['gpu_utilization']}\n"
            f"Utilisation MÃ©moire: {current['memory_utilization']}\n"
            f"\n"
            f"ğŸ“Š MOYENNES (5 DERNIÃˆRES MINUTES)\n"
            f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            f"TempÃ©rature moy.: {averages['temperature']}\n"
            f"VRAM moy.: {averages['vram_usage']}\n"
            f"Utilisation GPU moy.: {averages['gpu_utilization']}\n"
            f"\n"
            f"ğŸ¯ PROFIL OPTIMAL RECOMMANDÃ‰\n"
            f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            f"Nom: {optimal['name']}\n"
            f"Description: {optimal['description']}\n"
            f"```"
        )
        
        await ctx.send(msg)
        
        # Configuration optimale
        config = optimal["config"]
        config_msg = (
            f"```\n"
            f"âš™ï¸ CONFIGURATION OPTIMALE\n"
            f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            f"Contexte: {config['n_ctx']:,} tokens\n"
            f"Couches GPU: {config['n_gpu_layers']}\n"
            f"Batch size: {config['n_batch']}\n"
            f"Threads: {config['n_threads']}\n"
            f"Flash attention: {'Oui' if config.get('flash_attn') else 'Non'}\n"
            f"Offload KQV: {'Oui' if config.get('offload_kqv') else 'Non'}\n"
            f"Use mmap: {'Oui' if config.get('use_mmap') else 'Non'}\n"
            f"Use mlock: {'Oui' if config.get('use_mlock') else 'Non'}\n"
            f"```"
        )
        
        await ctx.send(config_msg)
        
        # Recommandations
        recommendations_msg = (
            f"ğŸ’¡ **RECOMMANDATIONS:**\n" + 
            "\n".join(f"â€¢ {rec}" for rec in recommendations)
        )
        
        await ctx.send(recommendations_msg)
        
        # Statut monitoring
        status_msg = (
            f"```\n"
            f"ğŸ“¡ STATUT MONITORING\n"
            f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            f"Monitoring actif: {'Oui' if monitoring['active'] else 'Non'}\n"
            f"Auto-optimisation: {'Oui' if monitoring['auto_optimization'] else 'Non'}\n"
            f"MÃ©triques collectÃ©es: {monitoring['metrics_collected']:,}\n"
            f"```"
        )
        
        await ctx.send(status_msg)
        
    except Exception as e:
        await ctx.send(f"âŒ Erreur gÃ©nÃ©ration rapport: {e}")

async def show_performance_metrics(ctx):
    """Affiche les mÃ©triques de performance temps rÃ©el"""
    try:
        metrics = model_manager.get_performance_metrics()
        
        if not metrics:
            await ctx.send("âŒ MÃ©triques de performance non disponibles")
            return
        
        vram = metrics["vram_usage"]
        msg = (
            f"```\n"
            f"âš¡ MÃ‰TRIQUES PERFORMANCE TEMPS RÃ‰EL\n"
            f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
            f"\n"
            f"ğŸ–¥ï¸ {metrics['gpu_name']}\n"
            f"\n"
            f"ğŸ’¾ VRAM\n"
            f"â”€â”€â”€â”€â”€â”€\n"
            f"UtilisÃ©e: {vram['used_mb']:,} MB\n"
            f"Totale: {vram['total_mb']:,} MB\n"
            f"Libre: {vram['free_mb']:,} MB\n"
            f"Usage: {vram['usage_percent']:.1f}%\n"
            f"\n"
            f"ğŸŒ¡ï¸ TEMPÃ‰RATURE & PERFORMANCE\n"
            f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            f"TempÃ©rature: {metrics['temperature_c']}Â°C\n"
            f"Consommation: {metrics['power_usage_w']}W\n"
            f"Utilisation GPU: {metrics['gpu_utilization']}%\n"
            f"Utilisation mÃ©moire: {metrics['memory_utilization']}%\n"
            f"\n"
            f"ğŸ• DerniÃ¨re mise Ã  jour: {metrics['timestamp']}\n"
            f"```"
        )
        
        # Barre de progression visuelle pour la VRAM
        vram_percent = vram['usage_percent']
        bar_length = 20
        filled = int(bar_length * vram_percent / 100)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
        
        bar_msg = f"**VRAM Usage:** `[{bar}]` **{vram_percent:.1f}%**"
        
        await ctx.send(msg)
        await ctx.send(bar_msg)
        
    except Exception as e:
        await ctx.send(f"âŒ Erreur affichage mÃ©triques: {e}")

async def toggle_auto_optimization(ctx, state):
    """Active/dÃ©sactive l'optimisation automatique"""
    try:
        if state is None:
            await ctx.send("âŒ SpÃ©cifiez l'Ã©tat: `!optimize auto on` ou `!optimize auto off`")
            return
        
        if state in ['on', 'oui', 'yes', '1', 'true']:
            enable = True
            state_text = "activÃ©e"
        elif state in ['off', 'non', 'no', '0', 'false']:
            enable = False
            state_text = "dÃ©sactivÃ©e"
        else:
            await ctx.send("âŒ Ã‰tat invalide. Utilisez `on` ou `off`")
            return
        
        success = model_manager.enable_auto_optimization(enable)
        
        if success:
            await ctx.send(f"âœ… Optimisation automatique {state_text} avec succÃ¨s")
        else:
            await ctx.send(f"âŒ Impossible de {state_text.replace('Ã©e', 'er')} l'optimisation automatique")
            
    except Exception as e:
        await ctx.send(f"âŒ Erreur configuration auto-optimisation: {e}")

async def optimize_for_task(ctx, task_type):
    """Optimise pour un type de tÃ¢che spÃ©cifique"""
    try:
        valid_tasks = ['chat', 'batch', 'long_context']
        
        if task_type not in valid_tasks:
            await ctx.send(f"âŒ Type de tÃ¢che invalide: `{task_type}`\n"
                          f"Types valides: {', '.join(valid_tasks)}")
            return
        
        task_descriptions = {
            'chat': 'conversation interactive (latence faible)',
            'batch': 'traitement en lot (throughput Ã©levÃ©)', 
            'long_context': 'contexte Ã©tendu (mÃ©moire maximale)'
        }
        
        await ctx.send(f"ğŸ”§ Optimisation pour tÃ¢che: **{task_descriptions[task_type]}**")
        
        success = model_manager.optimize_for_task(task_type)
        
        if success:
            current_profile = model_manager.get_current_profile()
            profile_name = current_profile['config']['name']
            
            msg = (
                f"âœ… **Optimisation appliquÃ©e avec succÃ¨s**\n"
                f"ğŸ¯ Profil sÃ©lectionnÃ©: **{profile_name}**\n"
                f"ğŸ“‹ OptimisÃ© pour: **{task_descriptions[task_type]}**\n"
                f"\n"
                f"ğŸ’¡ Utilisez `!optimize current` pour voir les dÃ©tails"
            )
            await ctx.send(msg)
        else:
            await ctx.send(f"âŒ Ã‰chec de l'optimisation pour la tÃ¢che `{task_type}`")
            
    except Exception as e:
        await ctx.send(f"âŒ Erreur optimisation tÃ¢che: {e}")